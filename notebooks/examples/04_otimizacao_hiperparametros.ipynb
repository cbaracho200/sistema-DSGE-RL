{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o de Hiperpar√¢metros e Valida√ß√£o Cruzada\n",
    "\n",
    "Este notebook demonstra t√©cnicas avan√ßadas de otimiza√ß√£o:\n",
    "\n",
    "- **Grid Search**: Busca exaustiva em grade de hiperpar√¢metros\n",
    "- **Random Search**: Busca aleat√≥ria mais eficiente\n",
    "- **Bayesian Optimization**: Otimiza√ß√£o inteligente\n",
    "- **Time Series Cross-Validation**: Valida√ß√£o temporal apropriada\n",
    "- **An√°lise de Learning Curves**: Diagn√≥stico de overfitting/underfitting\n",
    "- **Feature Engineering Autom√°tica**: Cria√ß√£o de features otimizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dados_realistas(n_periodos=180, seed=42):\n",
    "    \"\"\"Gera dataset sint√©tico.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    datas = pd.date_range(start='2010-01-01', periods=n_periodos, freq='M')\n",
    "    t = np.arange(n_periodos)\n",
    "    \n",
    "    pib = 2000 + 15*t + 200*np.sin(2*np.pi*t/48) + np.random.normal(0, 50, n_periodos)\n",
    "    \n",
    "    selic = np.zeros(n_periodos)\n",
    "    selic[0] = 10.0\n",
    "    for i in range(1, n_periodos):\n",
    "        shock = np.random.normal(0, 0.3)\n",
    "        if i % 30 == 0:\n",
    "            shock += np.random.choice([-2, 2])\n",
    "        selic[i] = np.clip(selic[i-1] + shock, 2.0, 20.0)\n",
    "    \n",
    "    ipca = np.zeros(n_periodos)\n",
    "    ipca[0] = 0.5\n",
    "    for i in range(1, n_periodos):\n",
    "        ipca[i] = 0.6*ipca[i-1] + 0.3 + np.random.normal(0, 0.2)\n",
    "        ipca[i] = np.clip(ipca[i], -1.0, 2.5)\n",
    "    \n",
    "    pib_norm = (pib - pib.mean()) / pib.std()\n",
    "    desemprego = 10.0 - 2*pib_norm + np.random.normal(0, 0.5, n_periodos)\n",
    "    desemprego = np.clip(desemprego, 4.0, 16.0)\n",
    "    \n",
    "    credito = 50000 + 400*t + 5000*pib_norm - 2000*(selic - selic.mean())/selic.std()\n",
    "    credito += np.random.normal(0, 2000, n_periodos)\n",
    "    \n",
    "    confianca = 100 + 15*pib_norm - 10*(desemprego - desemprego.mean())/desemprego.std()\n",
    "    confianca += np.random.normal(0, 5, n_periodos)\n",
    "    \n",
    "    idci_raw = (0.3*pib_norm - 0.2*(selic - selic.mean())/selic.std() + \n",
    "                0.2*confianca/20 - 0.15*(desemprego - desemprego.mean())/desemprego.std() +\n",
    "                0.15*(credito - credito.mean())/credito.std())\n",
    "    idci_vix = 5 + 2*idci_raw + np.random.normal(0, 0.3, n_periodos)\n",
    "    idci_vix = np.clip(idci_vix, 0, 10)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'data': datas,\n",
    "        'pib_real': pib,\n",
    "        'taxa_selic': selic,\n",
    "        'ipca': ipca,\n",
    "        'taxa_desemprego': desemprego,\n",
    "        'credito_imobiliario': credito,\n",
    "        'confianca_consumidor': confianca,\n",
    "        'IDCI_VIX': idci_vix\n",
    "    })\n",
    "    \n",
    "    df.set_index('data', inplace=True)\n",
    "    return df\n",
    "\n",
    "df = gerar_dados_realistas(n_periodos=180)\n",
    "print(f\"Dataset: {len(df)} observa√ß√µes\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "feature_cols = ['pib_real', 'taxa_selic', 'ipca', 'taxa_desemprego', \n",
    "                'credito_imobiliario', 'confianca_consumidor']\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['IDCI_VIX'].values\n",
    "\n",
    "# Split: 80% treino, 20% teste\n",
    "train_size = int(0.8 * len(df))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Treino: {len(X_train)} | Teste: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Cross-Validation\n",
    "\n",
    "Valida√ß√£o cruzada apropriada para s√©ries temporais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Time Series Split\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Visualizar os splits\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "    # Treino\n",
    "    ax.barh(i, len(train_idx), left=train_idx[0], height=0.4, \n",
    "            color='blue', alpha=0.6, label='Treino' if i == 0 else '')\n",
    "    # Valida√ß√£o\n",
    "    ax.barh(i, len(val_idx), left=val_idx[0], height=0.4, \n",
    "            color='orange', alpha=0.6, label='Valida√ß√£o' if i == 0 else '')\n",
    "    \n",
    "    # Anota√ß√µes\n",
    "    ax.text(train_idx[-1] + 2, i, f'Train: {len(train_idx)}', \n",
    "            va='center', fontsize=9)\n",
    "    ax.text(val_idx[-1] + 2, i, f'Val: {len(val_idx)}', \n",
    "            va='center', fontsize=9, color='orange')\n",
    "\n",
    "ax.set_yticks(range(n_splits))\n",
    "ax.set_yticklabels([f'Split {i+1}' for i in range(n_splits)])\n",
    "ax.set_xlabel('√çndice de Observa√ß√£o', fontsize=12)\n",
    "ax.set_title('Time Series Cross-Validation - Splits', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfigura√ß√£o: {n_splits} splits para valida√ß√£o cruzada temporal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Search - ARIMA\n",
    "\n",
    "Busca exaustiva dos melhores par√¢metros ARIMA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting.arima import ARIMAForecaster\n",
    "from evaluation.metrics import calculate_metrics\n",
    "\n",
    "# Grade de hiperpar√¢metros para ARIMA\n",
    "arima_param_grid = {\n",
    "    'p': [1, 2, 3],\n",
    "    'd': [0, 1],\n",
    "    'q': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Grid Search com CV temporal\n",
    "def grid_search_arima(X, y, param_grid, cv):\n",
    "    \"\"\"Grid search para ARIMA com CV temporal.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Criar todas combina√ß√µes\n",
    "    from itertools import product\n",
    "    combinations = list(product(param_grid['p'], param_grid['d'], param_grid['q']))\n",
    "    \n",
    "    print(f\"Testando {len(combinations)} combina√ß√µes de par√¢metros...\\n\")\n",
    "    \n",
    "    for p, d, q in combinations:\n",
    "        order = (p, d, q)\n",
    "        cv_scores = []\n",
    "        \n",
    "        # CV para cada combina√ß√£o\n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            y_train_cv = y[train_idx]\n",
    "            y_val_cv = y[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = ARIMAForecaster(order=order)\n",
    "                model.fit(y_train_cv)\n",
    "                y_pred = model.forecast(len(y_val_cv))\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "                cv_scores.append(rmse)\n",
    "            except:\n",
    "                cv_scores.append(np.inf)\n",
    "        \n",
    "        mean_score = np.mean(cv_scores)\n",
    "        std_score = np.std(cv_scores)\n",
    "        \n",
    "        results.append({\n",
    "            'order': order,\n",
    "            'p': p, 'd': d, 'q': q,\n",
    "            'mean_rmse': mean_score,\n",
    "            'std_rmse': std_score,\n",
    "            'cv_scores': cv_scores\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Executar grid search\n",
    "print(\"Executando Grid Search para ARIMA...\\n\")\n",
    "arima_results = grid_search_arima(X_train, y_train, arima_param_grid, tscv)\n",
    "\n",
    "# Ordenar por performance\n",
    "arima_results = arima_results.sort_values('mean_rmse')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 10 Melhores Configura√ß√µes ARIMA:\")\n",
    "print(\"=\"*80)\n",
    "print(arima_results.head(10)[['order', 'mean_rmse', 'std_rmse']])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "best_arima_order = arima_results.iloc[0]['order']\n",
    "print(f\"\\nüèÜ Melhor configura√ß√£o ARIMA: {best_arima_order}\")\n",
    "print(f\"   RMSE m√©dio: {arima_results.iloc[0]['mean_rmse']:.4f} ¬± {arima_results.iloc[0]['std_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados do grid search\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap: p vs q (m√©dia sobre d)\n",
    "pivot_data = arima_results.groupby(['p', 'q'])['mean_rmse'].mean().unstack()\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            ax=axes[0], cbar_kws={'label': 'RMSE M√©dio'})\n",
    "axes[0].set_title('Grid Search ARIMA: p vs q\\n(m√©dia sobre d)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('q (MA order)', fontsize=11)\n",
    "axes[0].set_ylabel('p (AR order)', fontsize=11)\n",
    "\n",
    "# Barplot: Top 10 configura√ß√µes\n",
    "top10 = arima_results.head(10).copy()\n",
    "top10['config'] = top10['order'].astype(str)\n",
    "axes[1].barh(range(10), top10['mean_rmse'].values, xerr=top10['std_rmse'].values,\n",
    "             alpha=0.7, color='steelblue')\n",
    "axes[1].set_yticks(range(10))\n",
    "axes[1].set_yticklabels(top10['config'].values)\n",
    "axes[1].set_xlabel('RMSE (mean ¬± std)', fontsize=11)\n",
    "axes[1].set_title('Top 10 Configura√ß√µes ARIMA', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Search - Random Forest\n",
    "\n",
    "Busca aleat√≥ria mais eficiente para modelos com muitos hiperpar√¢metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting.random_forest import RandomForestForecaster\n",
    "\n",
    "# Distribui√ß√µes de hiperpar√¢metros\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 200, 300],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'lags': [3, 5, 7, 10]\n",
    "}\n",
    "\n",
    "def random_search_rf(X, y, param_dist, cv, n_iter=50, random_state=42):\n",
    "    \"\"\"Random search para Random Forest.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testando {n_iter} combina√ß√µes aleat√≥rias...\\n\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Amostra aleat√≥ria de par√¢metros\n",
    "        params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "        \n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            X_train_cv = X[train_idx]\n",
    "            y_train_cv = y[train_idx]\n",
    "            X_val_cv = X[val_idx]\n",
    "            y_val_cv = y[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = RandomForestForecaster(\n",
    "                    n_estimators=params['n_estimators'],\n",
    "                    max_depth=params['max_depth'],\n",
    "                    min_samples_split=params['min_samples_split'],\n",
    "                    min_samples_leaf=params['min_samples_leaf'],\n",
    "                    max_features=params['max_features'],\n",
    "                    lags=params['lags'],\n",
    "                    random_state=random_state\n",
    "                )\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                y_pred = model.forecast(X_val_cv)\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "                cv_scores.append(rmse)\n",
    "            except Exception as e:\n",
    "                cv_scores.append(np.inf)\n",
    "        \n",
    "        mean_score = np.mean(cv_scores)\n",
    "        std_score = np.std(cv_scores)\n",
    "        \n",
    "        result = params.copy()\n",
    "        result['mean_rmse'] = mean_score\n",
    "        result['std_rmse'] = std_score\n",
    "        result['iteration'] = i\n",
    "        results.append(result)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Itera√ß√£o {i+1}/{n_iter} conclu√≠da | Melhor RMSE: {min([r['mean_rmse'] for r in results]):.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Executar random search\n",
    "print(\"Executando Random Search para Random Forest...\\n\")\n",
    "rf_results = random_search_rf(X_train, y_train, rf_param_distributions, tscv, n_iter=50)\n",
    "\n",
    "# Ordenar\n",
    "rf_results = rf_results.sort_values('mean_rmse')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 5 Melhores Configura√ß√µes Random Forest:\")\n",
    "print(\"=\"*80)\n",
    "print(rf_results.head()[['n_estimators', 'max_depth', 'lags', 'mean_rmse', 'std_rmse']])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "best_rf_params = rf_results.iloc[0].to_dict()\n",
    "print(f\"\\nüèÜ Melhor configura√ß√£o Random Forest:\")\n",
    "for key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features', 'lags']:\n",
    "    print(f\"   {key}: {best_rf_params[key]}\")\n",
    "print(f\"   RMSE: {best_rf_params['mean_rmse']:.4f} ¬± {best_rf_params['std_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar converg√™ncia do random search\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Converg√™ncia ao longo das itera√ß√µes\n",
    "rf_results_sorted = rf_results.sort_values('iteration')\n",
    "best_so_far = rf_results_sorted['mean_rmse'].cummin()\n",
    "axes[0, 0].plot(rf_results_sorted['iteration'], rf_results_sorted['mean_rmse'], \n",
    "                'o', alpha=0.4, label='Cada itera√ß√£o')\n",
    "axes[0, 0].plot(rf_results_sorted['iteration'], best_so_far, \n",
    "                '-r', linewidth=2, label='Melhor at√© o momento')\n",
    "axes[0, 0].set_xlabel('Itera√ß√£o', fontsize=11)\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=11)\n",
    "axes[0, 0].set_title('Converg√™ncia do Random Search', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Impacto de n_estimators\n",
    "axes[0, 1].scatter(rf_results['n_estimators'], rf_results['mean_rmse'], alpha=0.6, s=50)\n",
    "axes[0, 1].set_xlabel('n_estimators', fontsize=11)\n",
    "axes[0, 1].set_ylabel('RMSE', fontsize=11)\n",
    "axes[0, 1].set_title('Impacto de n_estimators', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Impacto de max_depth\n",
    "rf_results_depth = rf_results.copy()\n",
    "rf_results_depth['max_depth'] = rf_results_depth['max_depth'].fillna(30)  # Para visualiza√ß√£o\n",
    "axes[1, 0].scatter(rf_results_depth['max_depth'], rf_results_depth['mean_rmse'], alpha=0.6, s=50)\n",
    "axes[1, 0].set_xlabel('max_depth', fontsize=11)\n",
    "axes[1, 0].set_ylabel('RMSE', fontsize=11)\n",
    "axes[1, 0].set_title('Impacto de max_depth', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Impacto de lags\n",
    "axes[1, 1].scatter(rf_results['lags'], rf_results['mean_rmse'], alpha=0.6, s=50)\n",
    "axes[1, 1].set_xlabel('lags', fontsize=11)\n",
    "axes[1, 1].set_ylabel('RMSE', fontsize=11)\n",
    "axes[1, 1].set_title('Impacto de lags', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves - Diagn√≥stico de Overfitting\n",
    "\n",
    "Avaliar se o modelo est√° com overfitting ou underfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model_class, model_params, X, y, cv, train_sizes=None):\n",
    "    \"\"\"\n",
    "    Plota learning curve para diagnosticar overfitting/underfitting.\n",
    "    \"\"\"\n",
    "    if train_sizes is None:\n",
    "        train_sizes = np.linspace(0.3, 1.0, 7)\n",
    "    \n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for size in train_sizes:\n",
    "        train_scores_size = []\n",
    "        val_scores_size = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            # Limitar tamanho do treino\n",
    "            n_samples = int(len(train_idx) * size)\n",
    "            train_idx_sub = train_idx[:n_samples]\n",
    "            \n",
    "            X_train_cv = X[train_idx_sub]\n",
    "            y_train_cv = y[train_idx_sub]\n",
    "            X_val_cv = X[val_idx]\n",
    "            y_val_cv = y[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = model_class(**model_params)\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                \n",
    "                # Score no treino\n",
    "                y_train_pred = model.forecast(X_train_cv)\n",
    "                train_rmse = np.sqrt(mean_squared_error(y_train_cv, y_train_pred))\n",
    "                \n",
    "                # Score na valida√ß√£o\n",
    "                y_val_pred = model.forecast(X_val_cv)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val_cv, y_val_pred))\n",
    "                \n",
    "                train_scores_size.append(train_rmse)\n",
    "                val_scores_size.append(val_rmse)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if train_scores_size:\n",
    "            train_scores.append(train_scores_size)\n",
    "            val_scores.append(val_scores_size)\n",
    "    \n",
    "    # Calcular m√©dias e desvios\n",
    "    train_mean = [np.mean(scores) for scores in train_scores]\n",
    "    train_std = [np.std(scores) for scores in train_scores]\n",
    "    val_mean = [np.mean(scores) for scores in val_scores]\n",
    "    val_std = [np.std(scores) for scores in val_scores]\n",
    "    \n",
    "    actual_sizes = [int(len(X) * size * 0.8) for size in train_sizes[:len(train_mean)]]\n",
    "    \n",
    "    return actual_sizes, train_mean, train_std, val_mean, val_std\n",
    "\n",
    "# Plotar learning curves para Random Forest\n",
    "print(\"Calculando learning curves...\\n\")\n",
    "\n",
    "rf_params_best = {\n",
    "    'n_estimators': int(best_rf_params['n_estimators']),\n",
    "    'max_depth': best_rf_params['max_depth'] if best_rf_params['max_depth'] is not None else None,\n",
    "    'lags': int(best_rf_params['lags']),\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "sizes, train_mean, train_std, val_mean, val_std = plot_learning_curve(\n",
    "    RandomForestForecaster, rf_params_best, X_train, y_train, tscv\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.plot(sizes, train_mean, 'o-', label='Treino', linewidth=2, markersize=8, color='blue')\n",
    "ax.fill_between(sizes, \n",
    "                np.array(train_mean) - np.array(train_std),\n",
    "                np.array(train_mean) + np.array(train_std),\n",
    "                alpha=0.2, color='blue')\n",
    "\n",
    "ax.plot(sizes, val_mean, 's-', label='Valida√ß√£o', linewidth=2, markersize=8, color='orange')\n",
    "ax.fill_between(sizes, \n",
    "                np.array(val_mean) - np.array(val_std),\n",
    "                np.array(val_mean) + np.array(val_std),\n",
    "                alpha=0.2, color='orange')\n",
    "\n",
    "ax.set_xlabel('Tamanho do Conjunto de Treino', fontsize=13)\n",
    "ax.set_ylabel('RMSE', fontsize=13)\n",
    "ax.set_title('Learning Curve - Random Forest', fontsize=15, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Diagn√≥stico\n",
    "gap = val_mean[-1] - train_mean[-1]\n",
    "if gap > 0.5:\n",
    "    diagnosis = \"‚ö†Ô∏è OVERFITTING: Grande gap entre treino e valida√ß√£o\"\n",
    "elif train_mean[-1] > 1.0:\n",
    "    diagnosis = \"‚ö†Ô∏è UNDERFITTING: Erro alto em treino e valida√ß√£o\"\n",
    "else:\n",
    "    diagnosis = \"‚úì BOM AJUSTE: Modelo bem calibrado\"\n",
    "\n",
    "ax.text(0.02, 0.98, diagnosis, transform=ax.transAxes, \n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDiagn√≥stico: {diagnosis}\")\n",
    "print(f\"Gap treino-valida√ß√£o: {gap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compara√ß√£o Final: Modelos Otimizados vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos otimizados no conjunto de teste\n",
    "print(\"Treinando modelos otimizados...\\n\")\n",
    "\n",
    "# ARIMA otimizado\n",
    "arima_opt = ARIMAForecaster(order=best_arima_order)\n",
    "arima_opt.fit(y_train)\n",
    "pred_arima_opt = arima_opt.forecast(len(y_test))\n",
    "\n",
    "# ARIMA baseline\n",
    "arima_baseline = ARIMAForecaster(order=(1, 1, 1))\n",
    "arima_baseline.fit(y_train)\n",
    "pred_arima_base = arima_baseline.forecast(len(y_test))\n",
    "\n",
    "# Random Forest otimizado\n",
    "rf_opt = RandomForestForecaster(\n",
    "    n_estimators=int(best_rf_params['n_estimators']),\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    lags=int(best_rf_params['lags']),\n",
    "    random_state=42\n",
    ")\n",
    "rf_opt.fit(X_train, y_train)\n",
    "pred_rf_opt = rf_opt.forecast(X_test)\n",
    "\n",
    "# Random Forest baseline\n",
    "rf_baseline = RandomForestForecaster(n_estimators=100, max_depth=10, lags=3, random_state=42)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "pred_rf_base = rf_baseline.forecast(X_test)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "results_comparison = {\n",
    "    'ARIMA Baseline (1,1,1)': calculate_metrics(y_test, pred_arima_base),\n",
    "    f'ARIMA Otimizado {best_arima_order}': calculate_metrics(y_test, pred_arima_opt),\n",
    "    'RF Baseline': calculate_metrics(y_test, pred_rf_base),\n",
    "    'RF Otimizado': calculate_metrics(y_test, pred_rf_opt)\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(results_comparison).T\n",
    "\n",
    "print(\"\\nCompara√ß√£o de Performance: Baseline vs Otimizado\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.round(4))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Calcular melhorias\n",
    "print(\"\\nMelhorias (RMSE):\")\n",
    "arima_improvement = (df_comparison.loc['ARIMA Baseline (1,1,1)', 'rmse'] - \n",
    "                    df_comparison.loc[f'ARIMA Otimizado {best_arima_order}', 'rmse']) / \\\n",
    "                    df_comparison.loc['ARIMA Baseline (1,1,1)', 'rmse'] * 100\n",
    "rf_improvement = (df_comparison.loc['RF Baseline', 'rmse'] - \n",
    "                 df_comparison.loc['RF Otimizado', 'rmse']) / \\\n",
    "                 df_comparison.loc['RF Baseline', 'rmse'] * 100\n",
    "\n",
    "print(f\"  ARIMA: {arima_improvement:+.2f}%\")\n",
    "print(f\"  Random Forest: {rf_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "test_dates = df.index[train_size:]\n",
    "\n",
    "# ARIMA\n",
    "axes[0].plot(test_dates, y_test, 'o-', label='Real', linewidth=3, markersize=7, color='black')\n",
    "axes[0].plot(test_dates, pred_arima_base, 's--', label='ARIMA Baseline (1,1,1)', \n",
    "            linewidth=2, markersize=5, alpha=0.7)\n",
    "axes[0].plot(test_dates, pred_arima_opt, '^-', label=f'ARIMA Otimizado {best_arima_order}', \n",
    "            linewidth=2, markersize=5, alpha=0.7)\n",
    "axes[0].set_title(f'ARIMA: Melhoria de {arima_improvement:.1f}%', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('IDCI-VIX', fontsize=11)\n",
    "axes[0].legend(loc='best', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "axes[1].plot(test_dates, y_test, 'o-', label='Real', linewidth=3, markersize=7, color='black')\n",
    "axes[1].plot(test_dates, pred_rf_base, 's--', label='RF Baseline', \n",
    "            linewidth=2, markersize=5, alpha=0.7)\n",
    "axes[1].plot(test_dates, pred_rf_opt, '^-', label='RF Otimizado', \n",
    "            linewidth=2, markersize=5, alpha=0.7)\n",
    "axes[1].set_title(f'Random Forest: Melhoria de {rf_improvement:.1f}%', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Data', fontsize=11)\n",
    "axes[1].set_ylabel('IDCI-VIX', fontsize=11)\n",
    "axes[1].legend(loc='best', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Impacto da Otimiza√ß√£o de Hiperpar√¢metros', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumo e Recomenda√ß√µes\n",
    "\n",
    "### Principais Resultados:\n",
    "\n",
    "1. **Grid Search (ARIMA)**: Busca exaustiva encontrou configura√ß√£o √≥tima\n",
    "2. **Random Search (RF)**: Explora√ß√£o eficiente de espa√ßo de hiperpar√¢metros\n",
    "3. **Cross-Validation Temporal**: Valida√ß√£o apropriada para s√©ries temporais\n",
    "4. **Learning Curves**: Diagn√≥stico de overfitting/underfitting\n",
    "5. **Melhorias Significativas**: Otimiza√ß√£o trouxe ganhos mensur√°veis\n",
    "\n",
    "### Melhores Pr√°ticas:\n",
    "\n",
    "- Use **Time Series CV** em vez de CV aleat√≥rio\n",
    "- **Grid Search** para poucos par√¢metros, **Random Search** para muitos\n",
    "- Monitore **learning curves** para detectar problemas\n",
    "- Compare sempre com **baseline simples**\n",
    "- Documente **hiperpar√¢metros finais** para reprodutibilidade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
