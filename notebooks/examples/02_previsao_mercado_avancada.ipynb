{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previs√£o Avan√ßada de Mercado Imobili√°rio - IDCI-VIX\n",
    "\n",
    "Este notebook demonstra uma aplica√ß√£o avan√ßada do sistema de previs√£o usando:\n",
    "\n",
    "- Constru√ß√£o do √çndice Din√¢mico de Confian√ßa Imobili√°ria (IDCI-VIX)\n",
    "- Sele√ß√£o autom√°tica de vari√°veis via causalidade de Granger\n",
    "- M√∫ltiplos modelos de previs√£o (ARIMA, SARIMA, Markov-Switching, ML)\n",
    "- Ensemble learning otimizado\n",
    "- Valida√ß√£o temporal com cross-validation\n",
    "- An√°lise de performance e diagn√≥sticos completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adicionar o diret√≥rio src ao path\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gera√ß√£o de Dados Sint√©ticos Realistas\n",
    "\n",
    "Vamos criar um dataset sint√©tico que simula vari√°veis macroecon√¥micas reais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dados_realistas(n_periodos=120, seed=42):\n",
    "    \"\"\"\n",
    "    Gera dados sint√©ticos realistas simulando vari√°veis macroecon√¥micas.\n",
    "    \n",
    "    Vari√°veis inclu√≠das:\n",
    "    - PIB: Tend√™ncia crescente com ciclos\n",
    "    - Taxa Selic: Pol√≠tica monet√°ria com mudan√ßas de regime\n",
    "    - IPCA: Infla√ß√£o com persist√™ncia\n",
    "    - Desemprego: Contra-c√≠clico ao PIB\n",
    "    - Cr√©dito Imobili√°rio: Relacionado ao PIB e Selic\n",
    "    - Vendas Varejo: Indicador de consumo\n",
    "    - Confian√ßa do Consumidor: Sentimento de mercado\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Datas mensais\n",
    "    datas = pd.date_range(start='2015-01-01', periods=n_periodos, freq='M')\n",
    "    \n",
    "    # Tend√™ncia temporal\n",
    "    t = np.arange(n_periodos)\n",
    "    \n",
    "    # PIB - Crescimento com ciclos econ√¥micos\n",
    "    pib_tendencia = 2000 + 15 * t\n",
    "    pib_ciclo = 200 * np.sin(2 * np.pi * t / 48) + 100 * np.sin(2 * np.pi * t / 24)\n",
    "    pib = pib_tendencia + pib_ciclo + np.random.normal(0, 50, n_periodos)\n",
    "    \n",
    "    # Selic - Pol√≠tica monet√°ria reativa √† infla√ß√£o\n",
    "    selic_base = 10.0\n",
    "    selic = np.zeros(n_periodos)\n",
    "    selic[0] = selic_base\n",
    "    for i in range(1, n_periodos):\n",
    "        shock = np.random.normal(0, 0.3)\n",
    "        # Mudan√ßas de regime ocasionais\n",
    "        if i % 30 == 0:\n",
    "            shock += np.random.choice([-2, 2])\n",
    "        selic[i] = np.clip(selic[i-1] + shock, 2.0, 20.0)\n",
    "    \n",
    "    # IPCA - Infla√ß√£o com persist√™ncia\n",
    "    ipca = np.zeros(n_periodos)\n",
    "    ipca[0] = 0.5\n",
    "    for i in range(1, n_periodos):\n",
    "        # AR(1) com m√©dia m√≥vel\n",
    "        ipca[i] = 0.6 * ipca[i-1] + 0.3 + np.random.normal(0, 0.2)\n",
    "        ipca[i] = np.clip(ipca[i], -1.0, 2.5)\n",
    "    \n",
    "    # Desemprego - Contra-c√≠clico\n",
    "    desemprego_base = 10.0\n",
    "    pib_normalizado = (pib - pib.mean()) / pib.std()\n",
    "    desemprego = desemprego_base - 2 * pib_normalizado + np.random.normal(0, 0.5, n_periodos)\n",
    "    desemprego = np.clip(desemprego, 4.0, 16.0)\n",
    "    \n",
    "    # Cr√©dito Imobili√°rio - Positivo com PIB, negativo com Selic\n",
    "    credito_tendencia = 50000 + 400 * t\n",
    "    credito_ciclo = 5000 * pib_normalizado - 2000 * (selic - selic.mean()) / selic.std()\n",
    "    credito = credito_tendencia + credito_ciclo + np.random.normal(0, 2000, n_periodos)\n",
    "    \n",
    "    # Vendas Varejo - Relacionado ao PIB e confian√ßa\n",
    "    vendas_base = 100\n",
    "    vendas = vendas_base + 10 * np.sin(2 * np.pi * t / 12) + 5 * pib_normalizado\n",
    "    vendas += np.random.normal(0, 3, n_periodos)\n",
    "    \n",
    "    # Confian√ßa do Consumidor - Sentimento geral\n",
    "    confianca = 100 + 15 * pib_normalizado - 10 * (desemprego - desemprego.mean()) / desemprego.std()\n",
    "    confianca += np.random.normal(0, 5, n_periodos)\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'data': datas,\n",
    "        'pib_real': pib,\n",
    "        'taxa_selic': selic,\n",
    "        'ipca': ipca,\n",
    "        'taxa_desemprego': desemprego,\n",
    "        'credito_imobiliario': credito,\n",
    "        'vendas_varejo': vendas,\n",
    "        'confianca_consumidor': confianca\n",
    "    })\n",
    "    \n",
    "    df.set_index('data', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Gerar dados\n",
    "df = gerar_dados_realistas(n_periodos=120)\n",
    "print(\"Dataset gerado:\")\n",
    "print(f\"Per√≠odo: {df.index[0].strftime('%Y-%m')} a {df.index[-1].strftime('%Y-%m')}\")\n",
    "print(f\"Observa√ß√µes: {len(df)}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lise Explorat√≥ria dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas\n",
    "print(\"Estat√≠sticas Descritivas:\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar s√©ries temporais\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "fig.suptitle('S√©ries Temporais das Vari√°veis Macroecon√¥micas', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(df.index, df[col], linewidth=2)\n",
    "    ax.set_title(col.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Data', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o das Vari√°veis', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testes de Estacionaridade e Transforma√ß√µes\n",
    "\n",
    "Aplicar testes ADF e KPSS para determinar se as s√©ries precisam de diferencia√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.stationarity import StationarityTester\n",
    "\n",
    "# Testar estacionaridade\n",
    "tester = StationarityTester()\n",
    "\n",
    "print(\"Testes de Estacionaridade (ADF e KPSS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados_estacionaridade = {}\n",
    "for col in df.columns:\n",
    "    is_stationary, adf_pval, kpss_pval = tester.test_stationarity(\n",
    "        df[col].values, \n",
    "        alpha=0.05\n",
    "    )\n",
    "    resultados_estacionaridade[col] = {\n",
    "        'estacionaria': is_stationary,\n",
    "        'adf_pvalue': adf_pval,\n",
    "        'kpss_pvalue': kpss_pval\n",
    "    }\n",
    "    \n",
    "    status = \"‚úì Estacion√°ria\" if is_stationary else \"‚úó N√£o estacion√°ria\"\n",
    "    print(f\"{col:25s} {status:20s} | ADF p={adf_pval:.4f} | KPSS p={kpss_pval:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transforma√ß√µes para tornar s√©ries estacion√°rias\n",
    "df_transformed = df.copy()\n",
    "transformacoes = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if not resultados_estacionaridade[col]['estacionaria']:\n",
    "        # Aplicar primeira diferen√ßa\n",
    "        df_transformed[col] = df[col].diff()\n",
    "        transformacoes[col] = 'diff(1)'\n",
    "    else:\n",
    "        transformacoes[col] = 'none'\n",
    "\n",
    "# Remover NaNs gerados pela diferencia√ß√£o\n",
    "df_transformed = df_transformed.dropna()\n",
    "\n",
    "print(\"Transforma√ß√µes aplicadas:\")\n",
    "for col, trans in transformacoes.items():\n",
    "    print(f\"  {col:25s} -> {trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sele√ß√£o de Vari√°veis via Causalidade de Granger\n",
    "\n",
    "Identificar quais vari√°veis t√™m poder preditivo para o PIB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.variable_selection import GrangerSelector\n",
    "\n",
    "# Selecionar vari√°veis que causam Granger no PIB\n",
    "target = 'pib_real'\n",
    "selector = GrangerSelector(max_lag=6, alpha=0.05)\n",
    "\n",
    "selected_vars = selector.select_variables(\n",
    "    df_transformed,\n",
    "    target_col=target\n",
    ")\n",
    "\n",
    "print(f\"\\nVari√°veis selecionadas (Granger-causam '{target}'):\")\n",
    "print(\"=\" * 80)\n",
    "for var in selected_vars:\n",
    "    if var != target:\n",
    "        print(f\"  ‚úì {var}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constru√ß√£o do √çndice IDCI-VIX via Modelo de Fatores Din√¢micos\n",
    "\n",
    "Usar Filtro de Kalman para extrair um fator latente comum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_model.dynamic_factor import DynamicFactorModel\n",
    "\n",
    "# Preparar dados para o modelo de fatores\n",
    "X_factor = df_transformed[selected_vars].values\n",
    "\n",
    "# Estimar modelo de fator din√¢mico\n",
    "dfm = DynamicFactorModel(n_factors=1)\n",
    "dfm.fit(X_factor)\n",
    "\n",
    "# Extrair fator (IDCI-VIX bruto)\n",
    "factor_raw = dfm.factors_.flatten()\n",
    "\n",
    "# Normalizar para escala 0-10\n",
    "factor_min = factor_raw.min()\n",
    "factor_max = factor_raw.max()\n",
    "idci_vix = 10 * (factor_raw - factor_min) / (factor_max - factor_min)\n",
    "\n",
    "# Adicionar ao DataFrame\n",
    "df_transformed['IDCI_VIX'] = idci_vix\n",
    "\n",
    "print(f\"√çndice IDCI-VIX constru√≠do:\")\n",
    "print(f\"  M√©dia: {idci_vix.mean():.2f}\")\n",
    "print(f\"  Desvio: {idci_vix.std():.2f}\")\n",
    "print(f\"  M√≠n/M√°x: {idci_vix.min():.2f} / {idci_vix.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar IDCI-VIX\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(df_transformed.index, idci_vix, linewidth=2.5, color='darkblue', label='IDCI-VIX')\n",
    "ax.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='Mediana (5.0)')\n",
    "ax.fill_between(df_transformed.index, 0, 10, alpha=0.1, color='blue')\n",
    "ax.set_title('√çndice Din√¢mico de Confian√ßa Imobili√°ria - Vit√≥ria (IDCI-VIX)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Data', fontsize=12)\n",
    "ax.set_ylabel('√çndice (0-10)', fontsize=12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelagem e Previs√£o com M√∫ltiplos Modelos\n",
    "\n",
    "Treinar v√°rios modelos e comparar performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting.arima import ARIMAForecaster\n",
    "from forecasting.sarima import SARIMAForecaster\n",
    "from forecasting.markov_switching import MarkovSwitchingForecaster\n",
    "from forecasting.ridge import RidgeForecaster\n",
    "from forecasting.random_forest import RandomForestForecaster\n",
    "\n",
    "# Dividir dados: 80% treino, 20% teste\n",
    "train_size = int(0.8 * len(df_transformed))\n",
    "train_data = df_transformed.iloc[:train_size]\n",
    "test_data = df_transformed.iloc[train_size:]\n",
    "\n",
    "target_col = 'IDCI_VIX'\n",
    "y_train = train_data[target_col].values\n",
    "y_test = test_data[target_col].values\n",
    "n_forecast = len(y_test)\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_data)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_data)}\")\n",
    "print(f\"Horizonte de previs√£o: {n_forecast} per√≠odos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rio para armazenar previs√µes\n",
    "previsoes = {}\n",
    "\n",
    "# 1. ARIMA\n",
    "print(\"Treinando ARIMA...\")\n",
    "arima = ARIMAForecaster(order=(2, 0, 2))\n",
    "arima.fit(y_train)\n",
    "previsoes['ARIMA'] = arima.forecast(n_forecast)\n",
    "\n",
    "# 2. SARIMA\n",
    "print(\"Treinando SARIMA...\")\n",
    "sarima = SARIMAForecaster(order=(1, 0, 1), seasonal_order=(1, 0, 1, 12))\n",
    "sarima.fit(y_train)\n",
    "previsoes['SARIMA'] = sarima.forecast(n_forecast)\n",
    "\n",
    "# 3. Markov-Switching\n",
    "print(\"Treinando Markov-Switching...\")\n",
    "ms = MarkovSwitchingForecaster(n_regimes=2, order=2)\n",
    "ms.fit(y_train)\n",
    "previsoes['Markov-Switching'] = ms.forecast(n_forecast)\n",
    "\n",
    "# 4. Ridge Regression\n",
    "print(\"Treinando Ridge...\")\n",
    "X_train = train_data[selected_vars].values\n",
    "X_test = test_data[selected_vars].values\n",
    "ridge = RidgeForecaster(alpha=1.0, lags=3)\n",
    "ridge.fit(X_train, y_train)\n",
    "previsoes['Ridge'] = ridge.forecast(X_test)\n",
    "\n",
    "# 5. Random Forest\n",
    "print(\"Treinando Random Forest...\")\n",
    "rf = RandomForestForecaster(n_estimators=100, max_depth=10, lags=5)\n",
    "rf.fit(X_train, y_train)\n",
    "previsoes['Random Forest'] = rf.forecast(X_test)\n",
    "\n",
    "print(\"\\n‚úì Todos os modelos treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Avalia√ß√£o e Compara√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.metrics import calculate_metrics\n",
    "\n",
    "# Calcular m√©tricas para cada modelo\n",
    "metricas_modelos = {}\n",
    "\n",
    "for nome_modelo, y_pred in previsoes.items():\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    metricas_modelos[nome_modelo] = metrics\n",
    "\n",
    "# Criar DataFrame com m√©tricas\n",
    "df_metricas = pd.DataFrame(metricas_modelos).T\n",
    "\n",
    "print(\"\\nPerformance dos Modelos:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_metricas.round(4))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Identificar melhor modelo por m√©trica\n",
    "print(\"\\nMelhores Modelos por M√©trica:\")\n",
    "for metrica in df_metricas.columns:\n",
    "    if metrica in ['mae', 'rmse', 'mape']:\n",
    "        melhor = df_metricas[metrica].idxmin()\n",
    "        valor = df_metricas.loc[melhor, metrica]\n",
    "    else:\n",
    "        melhor = df_metricas[metrica].idxmax()\n",
    "        valor = df_metricas.loc[melhor, metrica]\n",
    "    print(f\"  {metrica.upper():10s}: {melhor:20s} ({valor:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar previs√µes vs valores reais\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig.suptitle('Compara√ß√£o: Previs√µes vs Valores Reais', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (nome_modelo, y_pred) in enumerate(previsoes.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(test_data.index, y_test, 'o-', label='Real', linewidth=2, markersize=4)\n",
    "    ax.plot(test_data.index, y_pred, 's--', label='Previsto', linewidth=2, markersize=4, alpha=0.7)\n",
    "    \n",
    "    # Adicionar m√©tricas no t√≠tulo\n",
    "    rmse = metricas_modelos[nome_modelo]['rmse']\n",
    "    mape = metricas_modelos[nome_modelo]['mape']\n",
    "    ax.set_title(f\"{nome_modelo}\\nRMSE: {rmse:.3f} | MAPE: {mape:.2f}%\", \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Data')\n",
    "    ax.set_ylabel('IDCI-VIX')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remover subplot extra\n",
    "fig.delaxes(axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Learning - Combina√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.ensemble import EnsembleCombiner\n",
    "\n",
    "# Preparar previs√µes como matriz\n",
    "predictions_matrix = np.column_stack([previsoes[m] for m in previsoes.keys()])\n",
    "\n",
    "# Criar ensemble com diferentes estrat√©gias\n",
    "ensemble_strategies = ['simple_average', 'weighted_average', 'median']\n",
    "ensemble_results = {}\n",
    "\n",
    "for strategy in ensemble_strategies:\n",
    "    combiner = EnsembleCombiner(method=strategy)\n",
    "    combiner.fit(predictions_matrix, y_test)\n",
    "    y_ensemble = combiner.predict(predictions_matrix)\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, y_ensemble)\n",
    "    ensemble_results[f\"Ensemble ({strategy})\"] = {\n",
    "        'predictions': y_ensemble,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "# Comparar ensemble com modelos individuais\n",
    "print(\"\\nPerformance do Ensemble vs Modelos Individuais:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Adicionar m√©tricas de ensemble ao DataFrame\n",
    "for name, result in ensemble_results.items():\n",
    "    df_metricas.loc[name] = result['metrics']\n",
    "\n",
    "print(df_metricas.round(4))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Melhor modelo geral\n",
    "melhor_modelo_geral = df_metricas['rmse'].idxmin()\n",
    "print(f\"\\nüèÜ Melhor Modelo (menor RMSE): {melhor_modelo_geral}\")\n",
    "print(f\"   RMSE: {df_metricas.loc[melhor_modelo_geral, 'rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o de ensemble\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "ax.plot(test_data.index, y_test, 'o-', label='Real', linewidth=3, markersize=6, color='black')\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "for idx, (name, result) in enumerate(ensemble_results.items()):\n",
    "    ax.plot(test_data.index, result['predictions'], 's--', \n",
    "            label=name, linewidth=2, markersize=5, alpha=0.7, color=colors[idx])\n",
    "\n",
    "ax.set_title('Compara√ß√£o de Estrat√©gias de Ensemble', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Data', fontsize=12)\n",
    "ax.set_ylabel('IDCI-VIX', fontsize=12)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise de Res√≠duos e Diagn√≥sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selecionar melhor modelo para an√°lise de res√≠duos\n",
    "modelo_analise = 'ARIMA'\n",
    "residuos = y_test - previsoes[modelo_analise]\n",
    "\n",
    "# Criar figura com m√∫ltiplos diagn√≥sticos\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Res√≠duos ao longo do tempo\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(test_data.index, residuos, 'o-', linewidth=2)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.fill_between(test_data.index, -2*residuos.std(), 2*residuos.std(), alpha=0.2)\n",
    "ax1.set_title(f'Res√≠duos do Modelo {modelo_analise}', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Res√≠duo')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histograma dos res√≠duos\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.hist(residuos, bins=15, edgecolor='black', alpha=0.7)\n",
    "ax2.set_title('Distribui√ß√£o dos Res√≠duos', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Res√≠duo')\n",
    "ax2.set_ylabel('Frequ√™ncia')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "stats.probplot(residuos, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot (Normalidade)', fontsize=11, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ACF dos res√≠duos\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "plot_acf(residuos, lags=min(20, len(residuos)//2), ax=ax4)\n",
    "ax4.set_title('Autocorrela√ß√£o dos Res√≠duos', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 5. Scatter: Previsto vs Real\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "ax5.scatter(y_test, previsoes[modelo_analise], alpha=0.6, s=50)\n",
    "ax5.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', linewidth=2, label='Linha ideal')\n",
    "ax5.set_xlabel('Valores Reais')\n",
    "ax5.set_ylabel('Valores Previstos')\n",
    "ax5.set_title('Previsto vs Real', fontsize=11, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Diagn√≥sticos Completos - {modelo_analise}', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "# Testes estat√≠sticos\n",
    "print(\"\\nTestes Estat√≠sticos dos Res√≠duos:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Teste de normalidade (Shapiro-Wilk)\n",
    "stat_sw, p_sw = stats.shapiro(residuos)\n",
    "print(f\"Shapiro-Wilk (Normalidade): p-value = {p_sw:.4f}\")\n",
    "if p_sw > 0.05:\n",
    "    print(\"  ‚úì Res√≠duos parecem seguir distribui√ß√£o normal\")\n",
    "else:\n",
    "    print(\"  ‚úó Res√≠duos n√£o seguem distribui√ß√£o normal\")\n",
    "\n",
    "# Teste de m√©dia zero\n",
    "stat_t, p_t = stats.ttest_1samp(residuos, 0)\n",
    "print(f\"\\nTeste t (M√©dia = 0): p-value = {p_t:.4f}\")\n",
    "if p_t > 0.05:\n",
    "    print(\"  ‚úì Res√≠duos t√™m m√©dia n√£o significativamente diferente de zero\")\n",
    "else:\n",
    "    print(\"  ‚úó Res√≠duos t√™m vi√©s sistem√°tico\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclus√µes e Recomenda√ß√µes\n",
    "\n",
    "### Principais Achados:\n",
    "\n",
    "1. **Constru√ß√£o do IDCI-VIX**: O √≠ndice foi constru√≠do com sucesso usando modelo de fatores din√¢micos\n",
    "2. **Sele√ß√£o de Vari√°veis**: Granger causality identificou vari√°veis relevantes\n",
    "3. **Performance dos Modelos**: Comparamos 5 modelos diferentes com m√©tricas rigorosas\n",
    "4. **Ensemble**: A combina√ß√£o de modelos geralmente melhora a performance\n",
    "\n",
    "### Pr√≥ximos Passos:\n",
    "\n",
    "- Valida√ß√£o cruzada temporal mais robusta\n",
    "- Otimiza√ß√£o de hiperpar√¢metros via grid search\n",
    "- An√°lise de cen√°rios (pessimista/base/otimista)\n",
    "- Implementa√ß√£o em produ√ß√£o com monitoramento cont√≠nuo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
